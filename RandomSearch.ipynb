{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MYPC\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "C:\\Users\\MYPC\\Anaconda3\\lib\\site-packages\\Pyro4\\util.py:839: UserWarning: msgpack serializer unavailable. requires msgpack 0.5.2+, found (0, 5, 1)\n",
      "  warnings.warn(\"msgpack serializer unavailable. requires msgpack 0.5.2+, found \" + str(msgpack.version))\n",
      "C:\\Users\\MYPC\\Anaconda3\\lib\\site-packages\\statsmodels\\compat\\pandas.py:56: FutureWarning: The pandas.core.datetools module is deprecated and will be removed in a future version. Please use the pandas.tseries module instead.\n",
      "  from pandas.core import datetools\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... loading data\n",
      "... done loading data\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import gzip\n",
    "import json\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "logging.basicConfig(level=logging.WARNING)\n",
    "\n",
    "try:\n",
    "    import keras\n",
    "    from keras.datasets import mnist\n",
    "    from keras.models import Sequential\n",
    "    from keras.layers import Dense, Dropout, Flatten\n",
    "    from keras.layers import Conv2D, MaxPooling2D\n",
    "    from keras import backend as K\n",
    "except:\n",
    "    raise ImportError(\"For this example you need to install keras.\")\n",
    "\n",
    "import hpbandster.core.nameserver as hpns\n",
    "\n",
    "from hpbandster.optimizers import RandomSearch\n",
    "\n",
    "import ConfigSpace as CS\n",
    "import ConfigSpace.hyperparameters as CSH\n",
    "from hpbandster.core.worker import Worker\n",
    "import argparse\n",
    "\n",
    "#from cnn_mnist_solution import mnist\n",
    "\n",
    "def onehot(labels):\n",
    "    \"\"\"this creates a one hot encoding from a flat vector:\n",
    "    i.e. given y = [0,2,1]\n",
    "     it creates y_one_hot = [[1,0,0], [0,0,1], [0,1,0]]\n",
    "    \"\"\"\n",
    "    classes = np.unique(labels)\n",
    "    n_classes = classes.size\n",
    "    one_hot_labels = np.zeros(labels.shape + (n_classes,))\n",
    "    for c in classes:\n",
    "        one_hot_labels[labels == c, c] = 1\n",
    "    return one_hot_labels\n",
    "\n",
    "def mnist(datasets_dir='./data'):\n",
    "    if not os.path.exists(datasets_dir):\n",
    "        os.mkdir(datasets_dir)\n",
    "    data_file = os.path.join(datasets_dir, 'mnist.pkl.gz')\n",
    "    if not os.path.exists(data_file):\n",
    "        print('... downloading MNIST from the web')\n",
    "        try:\n",
    "            import urllib\n",
    "            urllib.urlretrieve('http://google.com')\n",
    "        except AttributeError:\n",
    "            import urllib.request as urllib\n",
    "        url = 'http://www.iro.umontreal.ca/~lisa/deep/data/mnist/mnist.pkl.gz'\n",
    "        urllib.urlretrieve(url, data_file)\n",
    "\n",
    "    print('... loading data')\n",
    "    # Load the dataset\n",
    "    f = gzip.open(data_file, 'rb')\n",
    "    try:\n",
    "        train_set, valid_set, test_set = pickle.load(f, encoding=\"latin1\")\n",
    "    except TypeError:\n",
    "        train_set, valid_set, test_set = pickle.load(f)\n",
    "    f.close()\n",
    "\n",
    "    test_x, test_y = test_set\n",
    "    test_x = test_x.astype('float32')\n",
    "    test_x = test_x.astype('float32').reshape(test_x.shape[0], 28, 28, 1)\n",
    "    test_y = test_y.astype('int32')\n",
    "    valid_x, valid_y = valid_set\n",
    "    valid_x = valid_x.astype('float32')\n",
    "    valid_x = valid_x.astype('float32').reshape(valid_x.shape[0], 28, 28, 1)\n",
    "    valid_y = valid_y.astype('int32')\n",
    "    train_x, train_y = train_set\n",
    "    train_x = train_x.astype('float32').reshape(train_x.shape[0], 28, 28, 1)\n",
    "    train_y = train_y.astype('int32')\n",
    "    print('... done loading data')\n",
    "    return train_x,onehot(train_y), valid_x, onehot(valid_y), test_x,onehot(test_y)\n",
    "\n",
    "def train_again(config,budget):\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Conv2D(config['num_filters'], kernel_size=(config['filter_size'],config['filter_size']),\n",
    "                         activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    model.add(Conv2D(config['num_filters'], kernel_size=(config['filter_size'], config['filter_size']),\n",
    "                         activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "      \n",
    "    model.add(Dropout(0.4))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dropout(0.4))\n",
    "    model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "    optimizer = keras.optimizers.SGD(lr=config['learning_rate'])\n",
    "\n",
    "    model.compile(loss=keras.losses.categorical_crossentropy,optimizer=optimizer,metrics=['accuracy'])\n",
    "\n",
    "    history=model.fit(self.x_train, self.y_train,\n",
    "                  batch_size=config[\"batch_size\"],epochs=epochs,verbose=0,validation_data=(self.x_test, self.y_test))\n",
    "\n",
    "    return history\n",
    "    \n",
    "class MyWorker(Worker):\n",
    "\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.x_train, self.y_train, self.x_valid, self.y_valid, self.x_test, self.y_test = mnist(\"./\")\n",
    "\n",
    "    def compute(self, config, budget,**kwargs):\n",
    "        \"\"\"\n",
    "        Evaluates the configuration on the defined budget and returns the validation performance.\n",
    "        Args:\n",
    "            config: dictionary containing the sampled configurations by the optimizer\n",
    "            budget: (float) amount of time/epochs/etc. the model can use to train\n",
    "        Returns:\n",
    "            dictionary with mandatory fields:\n",
    "                'loss' (scalar)\n",
    "                'info' (dict)\n",
    "        \"\"\"\n",
    "        lr = config[\"learning_rate\"]\n",
    "        num_filters = config[\"num_filters\"]\n",
    "        batch_size = config[\"batch_size\"]\n",
    "        epochs = budget\n",
    "\n",
    "        # TODO: train and validate your convolutional neural networks here\n",
    "        \n",
    "        model = Sequential()\n",
    "\n",
    "        model.add(Conv2D(config['num_filters'], kernel_size=(config['filter_size'],config['filter_size']),\n",
    "                         activation='relu'))\n",
    "        model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "        model.add(Conv2D(config['num_filters'], kernel_size=(config['filter_size'], config['filter_size']),\n",
    "                         activation='relu'))\n",
    "        model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "        \n",
    "        \n",
    "        \n",
    "        model.add(Dropout(0.4))\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(128, activation='relu'))\n",
    "        model.add(Dropout(0.4))\n",
    "        model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "\n",
    "        optimizer = keras.optimizers.SGD(lr=config['learning_rate'])\n",
    "\n",
    "        model.compile(loss=keras.losses.categorical_crossentropy,optimizer=optimizer,metrics=['accuracy'])\n",
    "\n",
    "        history=model.fit(self.x_train, self.y_train,\n",
    "                  batch_size=config[\"batch_size\"],epochs=epochs,verbose=0,validation_data=(self.x_test, self.y_test))\n",
    "\n",
    "        train_score = model.evaluate(self.x_train, self.y_train, verbose=0)\n",
    "        val_score = model.evaluate(self.x_valid, self.y_valid, verbose=0)\n",
    "        test_score = model.evaluate(self.x_test, self.y_test, verbose=0)\n",
    "        validation_error=1-val_score[1]\n",
    "\n",
    "        # TODO: We minimize so make sure you return the validation error here\n",
    "        return ({\n",
    "            'loss': validation_error,  # this is the a mandatory field to run hyperband\n",
    "            'info': {'test accuracy': test_score[1],\n",
    "                        'train accuracy': train_score[1],\n",
    "                        'validation accuracy': val_score[1],\n",
    "                        'number of parameters': model.count_params(),\n",
    "                \n",
    "            }  # can be used for any user-defined information - also mandatory\n",
    "        })\n",
    "\n",
    "    @staticmethod\n",
    "    def get_configspace():\n",
    "        \n",
    "        # TODO: Implement configuration space here. \n",
    "        #See https://github.com/automl/HpBandSter/blob/master/hpbandster/examples/example_5_keras_worker.py  for an example\n",
    "      \n",
    "        cs = CS.ConfigurationSpace()\n",
    "\n",
    "        lr = CSH.UniformFloatHyperparameter('learning_rate', lower=0.0001, upper=0.1, default_value='0.1', log=True)\n",
    "\n",
    "        # For demonstration purposes, we add different optimizers as categorical hyperparameters.\n",
    "        # To show how to use conditional hyperparameters with ConfigSpace, we'll add the optimizers 'Adam' and 'SGD'.\n",
    "        # SGD has a different parameter 'momentum'.\n",
    "        #optimizer = CSH.CategoricalHyperparameter('optimizer', ['Adam', 'SGD'])\n",
    "\n",
    "        #sgd_momentum = CSH.UniformFloatHyperparameter('sgd_momentum', lower=0.0, upper=0.99, default_value=0.9, log=False)\n",
    "\n",
    "        #cs.add_hyperparameters([lr, optimizer, sgd_momentum])\n",
    "        cs.add_hyperparameters([lr])\n",
    "        \n",
    "        batch_size = CSH.UniformIntegerHyperparameter('batch_size', lower=16, upper=128, default_value=16, log=True)\n",
    "        cs.add_hyperparameters([batch_size])\n",
    "\n",
    "        #num_conv_layers =  CSH.UniformIntegerHyperparameter('num_conv_layers', lower=1, upper=3, default_value=2)\n",
    "\n",
    "        num_filters = CSH.UniformIntegerHyperparameter('num_filters', lower=8, upper=64, default_value=16, log=True)\n",
    "        cs.add_hyperparameters([num_filters])\n",
    "        \n",
    "        filter_size = CSH.CategoricalHyperparameter('filter_size', [3,5])\n",
    "        cs.add_hyperparameters([filter_size])\n",
    "\n",
    "\n",
    "        #dropout_rate = CSH.UniformFloatHyperparameter('dropout_rate', lower=0.0, upper=0.9, default_value=0.5, log=False)\n",
    "        #num_fc_units = CSH.UniformIntegerHyperparameter('num_fc_units', lower=8, upper=256, default_value=32, log=True)\n",
    "\n",
    "        #cs.add_hyperparameters([dropout_rate, num_fc_units])\n",
    "\n",
    "\n",
    "        # The hyperparameter sgd_momentum will be used,if the configuration\n",
    "        # contains 'SGD' as optimizer.\n",
    "        #cond = CS.EqualsCondition(sgd_momentum, optimizer, 'SGD')\n",
    "        #cs.add_condition(cond)\n",
    "\n",
    "        # You can also use inequality conditions:\n",
    "        #cond = CS.GreaterThanCondition(num_filters_2, num_conv_layers, 1)\n",
    "        #cs.add_condition(cond)\n",
    "\n",
    "        #cond = CS.GreaterThanCondition(num_filters_3, num_conv_layers, 2)\n",
    "        #cs.add_condition(cond)\n",
    "\n",
    "        return cs\n",
    "\n",
    "       \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "parser = argparse.ArgumentParser(description='Example 1 - sequential and local execution.')\n",
    "parser.add_argument('--budget', type=float,\n",
    "                    help='Maximum budget used during the optimization, i.e the number of epochs.', default=12)\n",
    "parser.add_argument('--n_iterations', type=int, help='Number of iterations performed by the optimizer', default=20)\n",
    "#args = parser.parse_args()\n",
    "\n",
    "# Step 1: Start a nameserver\n",
    "# Every run needs a nameserver. It could be a 'static' server with a\n",
    "# permanent address, but here it will be started for the local machine with the default port.\n",
    "# The nameserver manages the concurrent running workers across all possible threads or clusternodes.\n",
    "# Note the run_id argument. This uniquely identifies a run of any HpBandSter optimizer.\n",
    "NS = hpns.NameServer(run_id='example1', host='127.0.0.1', port=None)\n",
    "NS.start()\n",
    "\n",
    "# Step 2: Start a worker\n",
    "# Now we can instantiate a worker, providing the mandatory information\n",
    "# Besides the sleep_interval, we need to define the nameserver information and\n",
    "# the same run_id as above. After that, we can start the worker in the background,\n",
    "# where it will wait for incoming configurations to evaluate.\n",
    "w = MyWorker(nameserver='127.0.0.1', run_id='example1')\n",
    "w.run(background=True)\n",
    "\n",
    "# Step 3: Run an optimizer\n",
    "# Now we can create an optimizer object and start the run.\n",
    "# Here, we run RandomSearch, but that is not essential.\n",
    "# The run method will return the `Result` that contains all runs performed.\n",
    "\n",
    "rs = RandomSearch(configspace=w.get_configspace(),\n",
    "                  run_id='example1', nameserver='127.0.0.1',\n",
    "                 # min_budget=int(args.budget), max_budget=int(args.budget))\n",
    "                  min_budget=int(6), max_budget=int(6))\n",
    "res = rs.run(n_iterations=50)\n",
    "# Step 4: Shutdown\n",
    "# After the optimizer run, we must shutdown the master and the nameserver.\n",
    "rs.shutdown(shutdown_workers=True)\n",
    "NS.shutdown()\n",
    "\n",
    "# Step 5: Analysis\n",
    "# Each optimizer returns a hpbandster.core.result.Result object.\n",
    "# It holds information about the optimization run like the incumbent (=best) configuration.\n",
    "# For further details about the Result object, see its documentation.\n",
    "# Here we simply print out the best config and some statistics about the performed runs.\n",
    "id2config = res.get_id2config_mapping()\n",
    "incumbent = res.get_incumbent_id()\n",
    "\n",
    "print('Best found configuration:', id2config[incumbent]['config'])\n",
    "\n",
    "\n",
    "# Plots the performance of the best found validation error over time\n",
    "all_runs = res.get_all_runs()\n",
    "# Let's plot the observed losses grouped by budget,\n",
    "import hpbandster.visualization as hpvis\n",
    "\n",
    "hpvis.losses_over_time(all_runs)\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.savefig(\"random_search.png\")\n",
    "\n",
    "# TODO: retrain the best configuration (called incumbent) and compute the test error\n",
    "#worker = MyWorker(nameserver='127.0.0.1', run_id='example1')\n",
    "#res = worker.compute(config=id2config[incumbent]['config'], budget=6, working_directory='.',test=True)\n",
    "res=train_again(config=id2config[incumbent]['config'], budget=6)\n",
    "print(\"Test Results:\")\n",
    "val_loss = res.history['val_loss']\n",
    "plt.plot(loss)\n",
    "print(res.history['info'])\n",
    "#Â© "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
