{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import argparse\n",
    "import gzip\n",
    "import json\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.INFO)\n",
    "\n",
    "\n",
    "def one_hot(labels):\n",
    "    \"\"\"this creates a one hot encoding from a flat vector:\n",
    "    i.e. given y = [0,2,1]\n",
    "     it creates y_one_hot = [[1,0,0], [0,0,1], [0,1,0]]\n",
    "    \"\"\"\n",
    "    classes = np.unique(labels)\n",
    "    n_classes = classes.size\n",
    "    one_hot_labels = np.zeros(labels.shape + (n_classes,))\n",
    "    for c in classes:\n",
    "        one_hot_labels[labels == c, c] = 1\n",
    "    return one_hot_labels\n",
    "\n",
    "\n",
    "def mnist(datasets_dir='./data'):\n",
    "    if not os.path.exists(datasets_dir):\n",
    "        os.mkdir(datasets_dir)\n",
    "    data_file = os.path.join(datasets_dir, 'mnist.pkl.gz')\n",
    "    if not os.path.exists(data_file):\n",
    "        print('... downloading MNIST from the web')\n",
    "        try:\n",
    "            import urllib\n",
    "            urllib.urlretrieve('http://google.com')\n",
    "        except AttributeError:\n",
    "            import urllib.request as urllib\n",
    "        url = 'http://www.iro.umontreal.ca/~lisa/deep/data/mnist/mnist.pkl.gz'\n",
    "        urllib.urlretrieve(url, data_file)\n",
    "\n",
    "    print('... loading data')\n",
    "    # Load the dataset\n",
    "    f = gzip.open(data_file, 'rb')\n",
    "    try:\n",
    "        train_set, valid_set, test_set = pickle.load(f, encoding=\"latin1\")\n",
    "    except TypeError:\n",
    "        train_set, valid_set, test_set = pickle.load(f)\n",
    "    f.close()\n",
    "\n",
    "    test_x, test_y = test_set\n",
    "    test_x = test_x.astype('float32')\n",
    "    test_x = test_x.astype('float32').reshape(test_x.shape[0], 28, 28, 1)\n",
    "    test_y = test_y.astype('int32')\n",
    "    valid_x, valid_y = valid_set\n",
    "    valid_x = valid_x.astype('float32')\n",
    "    valid_x = valid_x.astype('float32').reshape(valid_x.shape[0], 28, 28, 1)\n",
    "    valid_y = valid_y.astype('int32')\n",
    "    train_x, train_y = train_set\n",
    "    train_x = train_x.astype('float32').reshape(train_x.shape[0], 28, 28, 1)\n",
    "    train_y = train_y.astype('int32')\n",
    "    print('... done loading data')\n",
    "    return train_x, train_y, valid_x, valid_y, test_x, test_y\n",
    "\n",
    "def unhot(one_hot_labels):\n",
    "    \"\"\" Invert a one hot encoding, creating a flat vector \"\"\"\n",
    "    return np.argmax(one_hot_labels, axis=-1)\n",
    "\n",
    "def cnn_network(features,labels,mode):\n",
    "    # Convolutional Layer #1\n",
    "    # Input Layer\n",
    "    input_layer = tf.reshape(features[\"x\"], [-1, 28, 28, 1])\n",
    "    conv1 = tf.layers.conv2d(\n",
    "              inputs=input_layer,\n",
    "              filters=16,\n",
    "              kernel_size=[5, 5],\n",
    "              padding=\"same\",\n",
    "              activation=tf.nn.relu)\n",
    "\n",
    "    # Pooling Layer #1\n",
    "    pool1 = tf.layers.max_pooling2d(inputs=conv1, pool_size=[2, 2], strides=2)\n",
    "    # Convolutional Layer #1\n",
    "    conv2 = tf.layers.conv2d(\n",
    "              inputs=pool1,\n",
    "              filters=16,\n",
    "              kernel_size=[5, 5],\n",
    "              padding=\"same\",\n",
    "              activation=tf.nn.relu)\n",
    "\n",
    "    # Pooling Layer #1\n",
    "    pool2 = tf.layers.max_pooling2d(inputs=conv2, pool_size=[2, 2], strides=2)\n",
    "        \n",
    "    # Dense Layer\n",
    "    #np.ravel(np.copy(pool2)) #    \n",
    "    pool2_flat = tf.reshape(pool2, [-1, 7 * 7 * 16])\n",
    "    dense = tf.layers.dense(inputs=pool2_flat, units=128, activation=tf.nn.relu)\n",
    "    dropout = tf.layers.dropout(inputs=dense, rate=0.4,training=True)#size: [batch_size,128]#, training=mode == tf.estimator.ModeKeys.TRAIN)\n",
    "            \n",
    "    # Logits Layer        \n",
    "    logits = tf.layers.dense(inputs=dropout, units=10)#size: [batch_size,10]  \n",
    "    \n",
    "    predictions = {\n",
    "      # Generate predictions (for PREDICT and EVAL mode)\n",
    "     \"classes\": tf.argmax(input=logits, axis=1),\n",
    "      # Add `softmax_tensor` to the graph. It is used for PREDICT and by the `logging_hook`.\n",
    "     \"probabilities\": tf.nn.softmax(logits, name=\"softmax_tensor\")\n",
    "       }\n",
    "    #if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "       # return tf.estimator.EstimatorSpec(mode=mode, predictions=predictions)\n",
    "        \n",
    "    # Calculate Loss\n",
    "    loss = tf.losses.sparse_softmax_cross_entropy(labels=labels, logits=logits)\n",
    "      \n",
    "    if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "        print(\"in training mode............\")\n",
    "        optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.0001)\n",
    "   \n",
    "        train_op  = optimizer.minimize(loss=loss,global_step=tf.train.get_global_step())\n",
    "        estimator  = tf.estimator.EstimatorSpec(tf.estimator.ModeKeys.TRAIN,loss=loss, train_op=train_op)\n",
    "        return estimator\n",
    "    \n",
    "    # Add evaluation metrics (for EVAL mode)\n",
    "    if mode == tf.estimator.ModeKeys.EVAL: \n",
    "        print(\"in validation mode............\")\n",
    "        eval_metric_ops = { \"accuracy\": tf.metrics.accuracy(labels=labels, predictions=predictions[\"classes\"])}\n",
    "        return tf.estimator.EstimatorSpec(mode=mode, loss=loss, eval_metric_ops=eval_metric_ops)\n",
    "    \n",
    "   \n",
    "    \n",
    "def train_and_validate(x_train, y_train, x_valid, y_valid, num_epochs, lr, num_filters, batch_size):\n",
    "    # TODO: train and validate your convolutional neural networks with the provided data and hyperparameters\n",
    "    #cnn_network(x_train,y_train,lr, num_filters)\n",
    "    #print(np.shape(x_train))\n",
    "    #print(np.shape(y_train))\n",
    "    #print(\"y train labels::\",y_train)\n",
    "    #print(\"y train labels::\")\n",
    "    \n",
    "    # Set up logging for predictions\n",
    "    tensors_to_log = {\"probabilities\": \"softmax_tensor\"}\n",
    "    logging_hook = tf.train.LoggingTensorHook(tensors=tensors_to_log, every_n_iter=20)\n",
    "    \n",
    "    # Create the Estimator\n",
    "    mnist_classifier = tf.estimator.Estimator(model_fn=cnn_network, model_dir=\"./mnist_convnet_modellearningrate0001-5x5\")\n",
    "    \n",
    "    # Train the model\n",
    "    train_input_fn = tf.estimator.inputs.numpy_input_fn( x={\"x\": x_train},    y=y_train,    batch_size=batch_size,   \n",
    "                                                        num_epochs=None,    shuffle=True)\n",
    "    \n",
    "    # Evaluate the model and print results\n",
    "    eval_input_fn = tf.estimator.inputs.numpy_input_fn(    x={\"x\": x_valid},  y=y_valid,  num_epochs=1, shuffle=False)\n",
    "        \n",
    "    learning_curve=[]\n",
    "    \n",
    "    for i in range(1,num_epochs):\n",
    "        train_results = mnist_classifier.train( input_fn = train_input_fn,  hooks=[logging_hook],steps=1)\n",
    "        print(\"training results\")\n",
    "        print(train_results)\n",
    "\n",
    "\n",
    "        eval_results = mnist_classifier.evaluate(input_fn=eval_input_fn)\n",
    "        print(\"eval results\")\n",
    "        print(eval_results)\n",
    "        learning_curve.append(eval_results)\n",
    "    \n",
    "    return learning_curve,mnist_classifier  # TODO: Return the validation error after each epoch (i.e learning curve) and your model\n",
    "\n",
    "\n",
    "def test(x_test, y_test, model):\n",
    "    # TODO: test your network here by evaluating it on the test data\n",
    "    eval_input_fn = tf.estimator.inputs.numpy_input_fn(    x={\"x\": x_test},  y=y_test,  num_epochs=1, shuffle=False)\n",
    "    \n",
    "    test_results = model.evaluate(input_fn=eval_input_fn)\n",
    "    \n",
    "    return test_results\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--output_path\", default=\"./\", type=str, nargs=\"?\",\n",
    "                        help=\"Path where the results will be stored\")\n",
    "    parser.add_argument(\"--input_path\", default=\"./\", type=str, nargs=\"?\",\n",
    "                        help=\"Path where the data is located. If the data is not available it will be downloaded first\")\n",
    "    parser.add_argument(\"--learning_rate\", default=1e-3, type=float, nargs=\"?\", help=\"Learning rate for SGD\")\n",
    "    parser.add_argument(\"--num_filters\", default=32, type=int, nargs=\"?\",\n",
    "                        help=\"The number of filters for each convolution layer\")\n",
    "    parser.add_argument(\"--batch_size\", default=128, type=int, nargs=\"?\", help=\"Batch size for SGD\")\n",
    "    parser.add_argument(\"--epochs\", default=12, type=int, nargs=\"?\",\n",
    "                        help=\"Determines how many epochs the network will be trained\")\n",
    "    parser.add_argument(\"--run_id\", default=0, type=int, nargs=\"?\",\n",
    "                        help=\"Helps to identify different runs of an experiments\")\n",
    "\n",
    "    #args = parser.parse_args()\n",
    "\n",
    "    # hyperparameters\n",
    "    lr = [0.1,0.01,0.001,0.0001] #args.learning_rate\n",
    "    def get_learning_rates():\n",
    "        return lr\n",
    "    \n",
    "    num_filters = 16 #args.num_filters\n",
    "    batch_size = 128 #args.batch_size\n",
    "    epochs = 2000 #args.epochs\n",
    "\n",
    "    # train and test convolutional neural network\n",
    "    x_train, y_train, x_valid, y_valid, x_test, y_test = mnist(\"./\")#args.input_path)\n",
    "\n",
    "    # for filter 5x5\n",
    "    learning_curve, model =  train_and_validate(x_train, y_train, x_valid, y_valid, epochs, lr, num_filters, batch_size)\n",
    "    test_error = test(x_test, y_test, model)\n",
    "    print(\"test results:\",test_error)\n",
    "       \n",
    "\n",
    "    # save results in a dictionary and write them into a .json file\n",
    "    results = dict()\n",
    "    results[\"lr\"] = lr\n",
    "    results[\"num_filters\"] = num_filters\n",
    "    results[\"batch_size\"] = batch_size\n",
    "    results[\"learning_curve\"] = learning_curve\n",
    "    results[\"test_error\"] = test_error\n",
    "\n",
    "    path = os.path.join(\"./\", \"results\")#args.output_path\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "\n",
    "    fname = os.path.join(path, \"results_run_%d.json\" )#% args.run_id)\n",
    "\n",
    "    fh = open(fname, \"w\")\n",
    "    #json.dump(results, fh)\n",
    "    fh.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot learning curve\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot(accuracy,max_epochs):\n",
    "    fig, ax = plt.subplots(1, 1)\n",
    "    ax.set_title('Validation accuracy')\n",
    "    ax.set_xlim([0, max_epochs+10])\n",
    "    ax.set_ylim([0, 1])\n",
    "    ax.set_xlabel('Epoch')\n",
    "    ax.set_ylabel('Accuracy')\n",
    "    ax.plot(accuracy, c='b', label=' learning rate=0.0001 \\n 16 5x5 filters\\n test accuracy={:.4f}'.format(test_error['accuracy']))\n",
    "    ax.legend(loc='upper right')\n",
    "    fig.canvas.draw()\n",
    "    \n",
    "accuracy = []\n",
    "\n",
    "for entry in learning_curve:\n",
    "    accuracy.append(entry['accuracy'])\n",
    "plot(accuracy,len(learning_curve))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
